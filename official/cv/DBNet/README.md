# DBNet

***

Paper: [Real-time Scene Text Detection with Differentiable Binarization](https://arxiv.org/abs/1911.08947)

Label: Text Detection

***

## Introduction

In recent years, segmentation based methods are very popular in scene text detection, because segmentation results can more accurately describe various shapes of scene text, such as curved text. However, post-processing of binarization is essential for segmentation based detection, which converts the probability graph generated by the segmentation method into the boundary box/region of the text. DBNet has proposed a module called differentiable binarization (DB), It can perform the binarization process in the segmentation network. The segmentation network optimized with the DB module can adaptively set the binarization threshold, which not only simplifies the post-processing, but also improves the performance of text detection.

![img](https://user-images.githubusercontent.com/22607038/142791306-0da6db2a-20a6-4a68-b228-64ff275f67b3.png)

## Dataset

Datasets used: [ICDAR2015](<https://rrc.cvc.uab.es/?ch=4&com=downloads>)

- Size: 132M
    - Training Set:
        - image: 88.5M(1000 images)
        - label: 157KB
    - Evaluation Set:
        - image: 43.3M(500 images)
        - label: 244KB
- Data format: image, label

## Environmental requirements

- Device（Ascend/GPU/CPU）
    - Use Ascend/GPU/CPU as hardware environment. Refer to [MindSpore](https://www.mindspore.cn/install) to install the runtime environment.
- MindSpore >= 1.9

```shell
git clone https://gitee.com/mindspore/models.git
cd models/official/cv/DBNet
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

## BenchMark

### Accuracy

| Model | pretrained Model | config | Train Set | Test Set | Device Num | Epoch | Test Size | Recall | Precision | Hmean | CheckPoint | Graph Train Log |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| DBNet-R18 | [R18](https://download.mindspore.cn/thirdparty/dbnet/resnet18-5c106cde.ckpt) | [cfg](config/dbnet/config_resnet18_1p.yaml) | ICDAR2015 Train | ICDAR2015 Test | 1 | 1200 | 736 | 78.63 | 84.21 | 81.32 | [download]() | [download]() |
| DBNet-R50 | [R50](https://download.mindspore.cn/thirdparty/dbnet/resnet50-19c8e357.ckpt) | [cfg](config/dbnet/config_resnet50_1p.yaml) | ICDAR2015 Train | ICDAR2015 Test | 1 | 1200 | 736 | 81.05 | 88.07 | 84.41 | [download]() | [download]() |

### Performance

| device | Model     | dataset   | Params(M) | PyNative train 1P bs=16 (ms/step) | PyNative train 8P bs=8 (ms/step) | PyNative infer(FPS)| Graph train 1P bs=16 (ms/step) | Graph train 8P bs=8 (ms/step) | Graph infer(FPS) |
| ------ | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- |
| Ascend | DBNet-R18 | ICDAR2015 |  11.78 M  |  370     |   530   |    -     |   224    |   195   |   40.62   |
|  GPU   | DBNet-R18 | ICDAR2015 |  11.78 M  |  710    |   880     |   -    |  560  |   435    |   30.97   |
| Ascend | DBNet-R50 | ICDAR2015 |  24.28 M  |  524      |   680     |   -     |  273    |   220    |   33.88   |
|  GPU   | DBNet-R50 | ICDAR2015 |  24.28 M  |  935      |   1054    |  -    |  730  |  547    |   23.95   |

This model is greatly affected by data processing, and the performance data on different machines fluctuate greatly. The above data are for reference.

The above data are tested at:

Ascend 910 32G 8 devices; Operating system: Euler2.8; Memory: 756 G; ARM 96 cores CPU;

GPU v100 PCIE 32G 8 devices; Operating system: Ubuntu 18.04; Memory: 502 G; x86 72 cores CPU.

## Quickly Start

```text
Parameter file description:
    config/config_base.yaml: Common parameter file, data set path, optimizer, training strategy and other parameters are usually set in this file
    config/dbnet/*.yaml: backbone training policy profile
    config/dbnet++/*.yaml: backbone training policy profile with dcn

notice：dbnet/*.yaml and /dbnet++/*.yaml parameters will cover config_base.yaml，Users can reasonably configure according to their needs

Single train backbone resnet18 with ICDAR2015 as an example
config/config_base.yaml configure training and reasoning data set path
train:
    img_dir: ICDAR2015/ch4_training_images
    gt_dir: ICDAR2015/ch4_training_localization_transcription_gt
eval:
    img_dir: ICDAR2015/ch4_test_images
    gt_dir: ICDAR2015/Challenge4_Test_Task1_GT

config/dbnet/config_resnet18_1p.yaml configure backone_ckpt pre-training path
backbone:
    backbone_ckpt: "./pretrained/resnet18-5c106cde.ckpt"
```

Run standalone:

```shell
bash run_standalone_train_ascend.sh [CONFIG_PATH] [DEVICE_ID]

# Ascend standalone train resnet18
bash run_standalone_train_ascend.sh ../config/dbnet/config_resnet18_1p.yaml 0

# Ascend standalone train resnet50
bash run_standalone_train_ascend.sh ../config/dbnet/config_resnet50_1p.yaml 0
```

Run distribution:

```shell
bash run_distribution_train_ascend.sh [RANK_TABLE_FILE] [DEVICE_NUM] [CONFIG_PATH]

# Ascend distribute train resnet18
bash run_standalone_train.sh hccl_8p.json 8 ../config/dbnet/config_resnet18_8p.yaml

# Ascend distribute train resnet50
bash run_standalone_train.sh hccl_8p.json 8 ../config/dbnet/config_resnet50_8p.yaml
```

Evaluation:

```shell
bash run_eval_ascend.sh [CONFIG_PATH] [CKPT_PATH] [DEVICE_ID]

# Ascend eval resnet18
bash run_eval_ascend.sh ../config/dbnet/config_resnet18_8p.yaml your_ckpt_path 0
```

If you need to modify the device or other configurations, please modify the corresponding items in the configuration file.

## Training

### Run standalone train

```shell
bash run_standalone_train_ascend.sh [CONFIG_PATH] [DEVICE_ID]
# CONFIG_PATH：The configuration file path is Ascend by default. If you need to modify it, please modify the device_target
# DEVICE_ID：Card number used for training
```

Executing the above command will run in the background. You can view the results through the train/log.txt file

After the training, you can find the checkpoint file in outputs.

### Run distribution train

```shell
bash run_distribution_train.sh [RANK_TABLE_FILE] [DEVICE_NUM] [CONFIG_PATH]
# RANK_TABLE_FILE: Build a distributed environment json file
# DEVICE_NUM：Number of cards used for training
# CONFIG_PATH：The configuration file path is Ascend by default. If you need to modify it, please modify the device_target
```

Executing the above command will run in the background. You can view the results through the [LOG_NAME].txt file.

## resume train

If you want to use the resume training function, you only need to resume in the config file_ Ckpt can be added to the ckpt path training that needs to continue training.

### ModelArts

1. Configure the ModelArts parameter in the config file:

- setting enable_modelarts=True
- Setting OBS dataset path data_url: <path of dataset in OBS>
- Set OBS training return path train_url: <Path of output file in OBS>

2. Referring to [ModelArts](https://support.huaweicloud.com/modelarts/index.html) executing training.

## Online Evaluation

### evaluation

```shell
bash run_eval_ascend.sh [CONFIG_PATH] [CKPT_PATH] [DEVICE_ID]
# CONFIG_PATH: The configuration file path, device target default is Ascend. If you need to modify it, please modify the device in the config file.
# DEVICE_ID: Device id used for training
```

Executing the above command will run in the background. You can view the results through the [LOG_NAME].txt file.

## Off-line Evaluation

### Export Process

```shell
python export.py --config_path=[CONFIG_PATH] --ckpt_path=[CKPT_PATH]
```

You can find the MINDIR file in current path.

### 310 Inference

Plaese refer to  [MindSpore Inference with C++ Deployment Guide](https://gitee.com/mindspore/models/blob/master/utils/cpp_infer/README_CN.md) to set environment variables.

```shell
bash scripts/run_cpp_infer.sh [MINDIR_PATH] [CONFIG_PATH] [OUTPUT_DIR] [DEVICE_TARGET] [DEVICE_ID]
# MINDIR_PATH: The path of MindIR file
# CONFIG_PATH: The configuration file path
# OUTPUT_DIR: Data preprocessing and result saving path
# DEVICE_TARGET: Should be in [Ascend, GPU, CPU], 310 Inference choose Ascend
# DEVICE_ID: Device id
```

## Disclaimers

Models only provide scripts for downloading and preprocessing public data sets. We do not own these datasets, nor are we responsible for their quality or maintenance. Please ensure that you have the permission to use the datasets under the permission of the datasets. The models trained on these datasets are only used for non-commercial research and teaching purposes.

To the dataset owner: If you do not want to include the data set in MindSpore models or want to update it in any way, we will delete or update all public content as required. Please contact us through Gitee. Thank you for your understanding and contribution to the community.

## Thank

This version of DBNet draws on some excellent open source projects, including:

https://github.com/MhLiao/DB.git

https://gitee.com/yanan0122/dbnet-and-dbnet_pp-by-mind-spore.git

## FAQ

Please refer to [Models FAQ](https://gitee.com/mindspore/models#FAQ) to find some common public questions.

Q: When there is not enough memory or too many threads with WARNING, how to solve it?

A: Adjust the `num_workers`, `prefetch_size`,  `max_rowsize` in configuration file.
Generally, excessive CPU consumption needs to be reduced `num_workers`; Excessive memory consumption needs to be reduced `num_workers`, `prefetch_size` and `max_rowsize`.

Q: What to do if loss does not converge in GPU environment?

A: Setting `mix_precision` to `False` in configuration file.

Q: Why TotalText has dataset interface but no configuration file?

A: TotalText needs to use the pretrained parameters on the SynthText dataset. Currently, no pre-trained parameter file on the SynthText dataset is provided.
