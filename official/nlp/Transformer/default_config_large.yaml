# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
data_url: ""
train_url: ""
checkpoint_url: ""
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
checkpoint_path: ''
device_target: Ascend
enable_profiling: False

# ==============================================================================
# config/cfg edict
transformer_network: 'large'
init_loss_scale_value: 1024
scale_factor: 2
scale_window: 2000
optimizer: 'Adam'
optimizer_adam_beta2: 0.997
#lr_schedule: edict({'learning_rate': 2.0, 'warmup_steps': 8000, 'start_decay_step': 16000, 'min_lr': 0.0,})

# transformer_net_cfg
batch_size: 96
seq_length: 128
vocab_size: 36560
hidden_size: 1024
num_hidden_layers: 6
num_attention_heads: 16
intermediate_size: 4096
hidden_act: "relu"
hidden_dropout_prob: 0.2
attention_probs_dropout_prob: 0.2
max_position_embeddings: 128
initializer_range: 0.02
label_smoothing: 0.1
dtype: mstype.float32
compute_type: mstype.float16

#eval_config/cfg edict
data_file: '/cache/data'
data_file_name: 'newstest2014-l128-mindrecord'
model_file: './transformer/transformer_trained.ckpt'
output_file: './output_eval.txt'

# transformer_net_cfg
batch_size_ev: 1
hidden_dropout_prob_ev: 0.0
attention_probs_dropout_prob_ev: 0.0
beam_width: 4
max_decode_length: 80
length_penalty_weight: 1.0

# ==============================================================================
# train.py / Argparse init.
distribute: "false"
epoch_size: 52
device_id: 0
device_num: 1
enable_lossscale: "true"
do_shuffle: "true"
enable_save_ckpt: "true"
save_checkpoint_steps: 2500
save_checkpoint_num: 30
save_checkpoint_path: "./"
bucket_boundaries: [16, 32, 48, 64, 128]
accumulation_steps: 1

# export.py /eval_config - transformer export
file_name: "transformer"
file_format: 'MINDIR'

#'postprocess / from eval_config'
result_dir: "./result_Files"

#'preprocess / from eval_config'
result_path: "./preprocess_Result/"

# src/process_output.py "recore nbest with smoothed sentence-level bleu."
vocab_file: ""

# create_data.py
input_file: ''
num_splits: 16
clip_to_max_len: False
max_seq_length: 128
bucket: [16, 32, 48, 64, 128]


---
# Config description for each option
enable_modelarts: 'Whether training on modelarts, default: False'
data_url: 'Dataset url for obs'
train_url: 'Training output url for obs'
data_path: 'Dataset path for local, it is better to use absolute path'
output_path: 'Training output path for local'
ann_file: 'Ann file, default is val.json.'

device_target: "device where the code will be implemented, default is Ascend"
checkpoint_path: "Checkpoint file path"
data_file: '/your/path/evaluation.mindrecord'
model_file: '/your/path/checkpoint_file'
output_file: './output_eval.txt'

distribute: "Run distribute, default is false."
epoch_size: "Epoch size, default is 52."
device_id: "Device id, default is 0."
device_num: "Use device nums, default is 1."
enable_lossscale: "Use lossscale or not, default is true."
do_shuffle: "Enable shuffle for dataset, default is true."
enable_save_ckpt: "Enable save checkpoint, default is true."
save_checkpoint_steps: "Save checkpoint steps, default is 2500."
save_checkpoint_num: "Save checkpoint numbers, default is 30."
save_checkpoint_path: "Save checkpoint file path"
bucket_boundaries: "sequence length for different bucket"
accumulation_steps: "Gradient accumulation steps, default is 1."

file_name: "output file name."
file_format: 'file format'
result_dir: "./result_Files"
result_path: "./preprocess_Result/"
vocab_file: "vocab file path."
input_file: 'Input raw text file (or comma-separated list of files).'
num_splits: 'The MindRecord file will be split into the number of partition.'
clip_to_max_len: 'clip sequences to maximum sequence length.'
max_seq_length: 'Maximum sequence length.'
bucket: 'bucket sequence length'

---
device_target: ["Ascend", "GPU", "CPU"]
file_format: ["AIR", "ONNX", "MINDIR"]
distribute: ['true', 'false']
enable_lossscale: ['true', 'false']
do_shuffle: ['true', 'false']
enable_save_ckpt: ['true', 'false']
