# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "Ascend"

# : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 
# config/cfg edict

batch_size: 1
epoch: 20
encoder_max_length: 512
decoder_max_length: 64
encoder_checkpoint_dir: 'roberta_base_ms/robarta_base_encoder_ms.ckpt'
decoder_checkpoint_dir: 'roberta_base_ms/robarta_base_decoder_ms.ckpt'
#model_checkpoint_save_path: 'output/checkpoint'
# lq edit 22-06-30
# model_file:  'output/roberta_seq2seq-1_64.ckpt'  #'roberta_base_ms/robarta_base_shared_ms.ckpt'
model_file: 'roberta_seq2seq_last.ckpt'
# ---
ckpt_save_dir: "./"
output_file: 'eval_result.txt'
save_checkpoint_steps: 50
keep_checkpoint_max: 5
device_id: 1
#data file
train_data_dir: 'data/train.mindrecord'
test_data_dir: 'data/test.mindrecord'
val_data_dir: 'data/validation.mindrecord'
test_data_json_dir: 'data/test.json'
vocab_file_path: 'roberta_base_ms/vocab.json'
profiler_file: './profiler_data'
checkpoint_path: ''
file_name: 'roberta_seq2seq0804.mindir'
enable_lossscale : True
#encoder_decoder model cfg
label_smoothing: 0.0
beam_width: 4
max_decode_length: 64
length_penalty_weight: 2.0
seq_length: 512
# lr_schedule: edict({'learning_rate': 2.0'warmup_steps': 8000'start_decay_step': 16000'min_lr': 0.0})

# optimizer and lr related
lr: 0.00003
enable_dynamic_lr: False
# loss related
loss_scale_value: 2
scale_factor: 2
scale_window: 1000

optimizer_cfg:
    optimizer: 'Adam'
    AdamWeightDecay:
        learning_rate: 0.01
        end_learning_rate: 0.0000000001  # 1e-10
        start_decay_step: 8000
        power: 1.0
        warmup_steps: 4000
        weight_decay: 0.01
        decay_filter: ['layernorm', 'bias']
        eps: 0.000001  # 1e-6
    Adam:
        learning_rate: 2
        end_learning_rate: 0.00000001  # 1e-8
        start_decay_step: 80000
        warmup_steps: 40000
        
    Lamb:
        learning_rate: 0.00002  # 2e-5,
        end_learning_rate: 0.0000000001  # 1e-10
        power: 1.0
        weight_decay: 0.01
        decay_filter: ['layernorm', 'bias']
    Momentum:
        learning_rate: 0.00002  # 2e-5
        momentum: 0.9

# roberta_net_cfg
roberta_network: 'base'
vocab_size: 50265  # pytorch 30522
hidden_size: 768
num_hidden_layers: 12
num_attention_heads: 12
intermediate_size: 3072
hidden_act: "gelu"
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
max_position_embeddings: 514  # 512
type_vocab_size: 16  # pytorch 2
initializer_range: 0.02
dtype: mstype.float32
compute_type: mstype.float32
pad_token_id: 1
bos_token_id: 0
eos_token_id: 2
is_decoder: False
add_cross_attention: False
tie_encoder_decoder: False
output_hidden_states: False
output_attentions: False
layer_norm_eps: 0.00001
use_cache: False





---
# Config description for each option
enable_modelarts: 'Whether training on modelartsdefault: False'
data_url: 'Dataset url for obs'
train_url: 'Training output url for obs'
data_path: 'Dataset path for localit is better to use absolute path'
output_path: 'Training output path for local'

device_target: "device where the code will be implementeddefault is Ascend"
checkpoint_path: "Checkpoint file path"
#data_file: '/your/path/evaluation.mindrecord'
model_file: '/your/path/checkpoint_file'
output_file: './output_eval.txt'
