# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unlesee you know exactly what you are doing)
use_modelarts: 0

# url for modelarts
data_url: ""
train_url: ""
outer_path: 's3://output/'

# mainly hyperparameters for training
aug: 1
epochs: 200
lr: 3e-4
batch_size: 2

# dataset options, we recommend the absolute path
train_data_path: "../dataset/LiTS2017/train"
val_data_path: "../dataset/LiTS2017/test"

# eval settings while training
eval_while_train: 1
eval_steps: 1
eval_start_epoch: 180

# checkpoint config while training
save_every: 1
is_save_on_master: 1
ckpt_save_max: 5
output_path: './output/'
resume_path: ""
resume_name: ""

# eval settings stand alone, other hyperparameters are shared with training
pretrain_path: "./"
ckpt_name: "best_map.ckpt"

# export settings stand alone, other hyperparameters are shared with training
export_batch_size: 2
image_height: 512
image_width: 512
ckpt_file: "./best_map.ckpt"
file_name: "unet3plus"
file_format: "MINDIR"

#310 infer
pre_result_path: "./preprocess_Result"
post_result_path: "./result_Files"
label_path: "./preprocess_Result/label.npy"

# dataset preprocess settings
source_path: "../origin_dataset/train/"
dest_path: "../dataset/LiTS2017"
buffer_path: "./buffer"

# ======================================================================================
# common options
device_target: 'Ascend'
is_distributed: 0
rank: 0
group_size: 1


---
# Help description for each configuration
use_modelarts: "Whether training on modelarts, 1 for True, 0 for False; default: 0"
data_url: "needed by modelarts, but we donot use it because the name is ambiguous"
train_url: "needed by modelarts, but we donot use it because the name is ambiguous"
outer_path: "obs path, to store e.g ckpt files"
aug: "Whether to apply data augmentation, 1 for True, 0 for False; default: 1"
epochs: "epochs for training"
lr: "lr"
batch_size: "batch_size"
train_data_path: "root path of train data"
val_data_path: "root path of val data"
eval_while_train: "Whether eval while training, 1 for True, 0 for False; default: 1"
eval_steps: "each N epochs we eval"
eval_start_epoch: "eval_start_epoch"
save_every: "save model at every x epoches"
is_save_on_master: "save ckpt on master or all rank"
ckpt_save_max: "Maximum number of checkpoint files can be saved"
output_path: "output_path,when use_modelarts is set 1, it would better be cache/output/"
resume_path: "put the path to resuming file if needed"
resume_name: "resuming file name"
pretrain_path: "path of the ckpt to eval"
ckpt_name: "name of the ckpt to eval"
export_batch_size: "batch size for export ckpt"
image_height: "image height for export ckpt"
image_width: "image width for export ckpt"
ckpt_file: "the ckpt to export"
file_name: "name of exported ckpt"
file_format: "file format, choose from ['MINDIR','AIR','ONNX']"
source_path: "the root path of MICCAI-LITS-2017 with nii format"
dest_path: "the root path of the folder that you want to store the data with png format"
buffer_path: "the buffer to process data, it will be REMOVED after process finished"
device_target: "device where the code will be implemented. (Default: Ascend)"
is_distributed: "if multi device"
rank: "local rank of distributed"
group_size: "world size of distributed"
