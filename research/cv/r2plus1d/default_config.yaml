# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unlesee you know exactly what you are doing)
use_modelarts: 0

# url for modelarts
data_url: ""
train_url: ""
outer_path: 's3://output/'

# mainly hyperparameters for training
num_classes: 101
layer_num: 34
epochs: 30
batch_size: 8
lr: 0.001
momentum: 0.9
weight_decay: 0.0005

# dataset options, we recommend the absolute path
dataset_root_path: "/home/mindspore/MIPT_team/Datasets/UCF101_img"
dataset_name: "ucf101"
val_mode: "val"
pack_file_name: ""

# eval settings while training
eval_while_train: 1
eval_steps: 1
eval_start_epoch: 20

# checkpoint config while training
save_every: 1
is_save_on_master: 1
ckpt_save_max: 5
output_path: './output/'
pretrain_path: "./"
ckpt_name: "r2plus1d_v1_resnet34_kinetics400.ckpt"
resume_path: ""
resume_name: ""
resume_epoch: 0

# eval or transfer learning settings, other hyperparameters are shared with training
eval_ckpt_path: "/data/code/"
eval_ckpt_name: "r2plus1d_best_map.ckpt"
eval_ckpt: ""

# export settings stand alone, other hyperparameters are shared with training
export_batch_size: 1
image_height: 112
image_width: 112
ckpt_file: "./r2plus1d_best_map.ckpt"
file_name: "r2plus1d"
file_format: "MINDIR"

# dataset preprocess settings
source_data_dir: "/home/mindspore/MIPT_team/Datasets/UCF101"
output_dir: "/home/mindspore/MIPT_team/Datasets/UCF101_img"
splited: 0

# ======================================================================================
# common options
device_target: ''
is_distributed: 0
rank: 0
group_size: 1


---
# Help description for each configuration
use_modelarts: "Whether training on modelarts, 1 for True, 0 for False; default: 0"
data_url: "needed by modelarts, but we donot use it because the name is ambiguous"
train_url: "needed by modelarts, but we donot use it because the name is ambiguous"
outer_path: "obs path, to store e.g ckpt files"
num_classes: "number of classes of the dataset "
layer_num: "number of layers of the network, choose from [18, 34]"
epochs: "epoch"
batch_size: "batch size"
lr: "learning rate"
momentum: "momentum of SGD optimizer"
weight_decay: "weight_decay of SGD optimizer"
dataset_root_path: "root path of dataset"
dataset_name: "name of dataset"
val_mode: "mode of validation, choose from ['val','test'], means clip and video accuracy respectively"
pack_file_name: "zip file name, used when the network is deployed on modelarts"
eval_while_train: "Whether eval while training, 1 for True, 0 for False; default: 1"
eval_steps: "each N epochs we eval"
eval_start_epoch: "eval_start_epoch"
save_every: "save model at every x epoches"
is_save_on_master: "save ckpt on master or all rank"
ckpt_save_max: "Maximum number of checkpoint files can be saved"
output_path: "output_path,when use_modelarts is set 1, it would better be cache/output/"
pretrain_path: "path of the ckpt used by transfer learning"
ckpt_name: "name of the ckpt used by transfer learning"
resume_path: "put the path to resuming file if needed"
resume_name: "resuming file name"
resume_epoch: "epoch of resuming operation"
eval_ckpt_path: "path of the ckpt to eval"
eval_ckpt_name: "name of the ckpt to eval"
export_batch_size: "batch size for export ckpt"
image_height: "image height for export ckpt"
image_width: "image width for export ckpt"
ckpt_file: "the ckpt to export"
file_name: "name of exported ckpt"
file_format: "file format, choose from ['MINDIR','AIR','ONNX']"
source_data_dir: "the root path of raw data"
output_dir: "the root path of the folder that you want to store the data with image format"
splited: "whether the dataset has been splited in advance, choose from [0, 1]"
device_target: "device where the code will be deployed. (Default: Ascend)"
is_distributed: "if multi device"
rank: "local rank of distributed"
group_size: "world size of distributed"
