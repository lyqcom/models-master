# Contents

- [Contents](#contents)
    - [YOLOv3-Res2Net Description](#yolov3-res2net-description)
    - [Model Architecture](#model-architecture)
    - [Dataset](#dataset)
    - [Environment Requirements](#environment-requirements)
    - [Quick Start](#quick-start)
    - [Script Description](#script-description)
        - [Script and Sample Code](#script-and-sample-code)
        - [Script Parameters](#script-parameters)
        - [Training Process](#training-process)
            - [Training](#training)
            - [Distributed Training](#distributed-training)
        - [Evaluation Process](#evaluation-process)
            - [Evaluation](#evaluation)
        - [Export MindIR](#export-mindir)
        - [Inference Process](#inference-process)
            - [Export MindIR](#export-mindir)
            - [Infer on Ascend310](#infer-on-ascend310)
            - [Result](#result)
        - [Post Training Quantization](#post-training-quantization)
    - [Model Description](#model-description)
        - [Performance](#performance)
            - [Evaluation Performance](#evaluation-performance)
            - [Inference Performance](#inference-performance)
    - [Description of Random Situation](#description-of-random-situation)
    - [ModelZoo Homepage](#modelzoo-homepage)

## [YOLOv3-Res2Net Description](#contents)

You only look once (YOLO) is a state-of-the-art, real-time object detection system. YOLOv3 is extremely fast and accurate.

Prior detection systems repurpose classifiers or localizers to perform detection. They apply the model to an image at multiple locations and scales. High scoring regions of the image are considered detections.
YOLOv3 use a totally different approach. It apply a single neural network to the full image. This network divides the image into regions and predicts bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities.

YOLOv3 uses a few tricks to improve training and increase performance, including: multi-scale predictions, a better backbone classifier, and more. The full details are in the paper!

[Paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf):  YOLOv3: An Incremental Improvement. Joseph Redmon, Ali Farhadi,
University of Washington

## [Model Architecture](#contents)

YOLOv3 use Res2Net for performing feature extraction.

## [Dataset](#contents)

Note that you can run the scripts based on the dataset mentioned in original paper or widely used in relevant domain/network architecture. In the following sections, we will introduce how to run the scripts using the related dataset below.

Dataset used: [coco2017](https://cocodataset.org/#download)

- Dataset size: 19G, 123,287 images, 80 object categories.
    - Train：13G, 82,783 images
    - Val：6G, 40,504 images
    - Annotations: 241M, Train/Val annotations
- The directory structure is as follows.

    ```text
        ├── dataset
            ├── coco2017
                ├── annotations
                │   ├─ instances_train2017.json
                │   └─ instances_val2017.json
                ├─ train2017
                │   ├─picture1.jpg
                │   ├─ ...
                │   └─picturen.jpg
                └─ val2017
                    ├─picture1.jpg
                    ├─ ...
                    └─picturen.jpg
    ```

- If the user uses his own data set, the data set format needs to be converted to coco data format,
  and the data in the JSON file should correspond to the image data one by one.
  After accessing user data, because the size and quantity of image data are different,
  lr, anchor_scale and training_shape may need to be adjusted appropriately.

## [Environment Requirements](#contents)

- Hardware（Ascend/GPU）
- Prepare hardware environment with Ascend or GPU processor.
- Framework
    - [MindSpore](https://www.mindspore.cn/install/en)
- For more information, please check the resources below：
    - [MindSpore Tutorials](https://www.mindspore.cn/tutorials/en/master/index.html)
    - [MindSpore Python API](https://www.mindspore.cn/docs/en/master/index.html)

## [Quick Start](#contents)

- After installing MindSpore via the official website, you can start training and evaluation in as follows. If running on GPU, please add `--device_target=GPU` in the python command or use the "_gpu" shell script ("xxx_gpu.sh").
- Prepare the backbone_res2net.ckpt and hccl_8p.json files, before run network.
    - Pretrained_backbone can use convert_checkpoint.py

      ```
      python convert_checkpoint.py (change the pretrained model path before using this command.)
      ```

    - Genatating hccl_8p.json, Run the script of utils/hccl_tools/hccl_tools.py.(Only useful for Ascend device)
      The following parameter "[0-8)" indicates that the hccl_8p.json file of cards 0 to 7 is generated.
        - The name of json file generated by this command is hccl_8p_01234567_{host_ip}.json. For convenience of expression, use hccl_8p.json represents the json file.

      ```
      python hccl_tools.py --device_num "[0,8)"
      ```

- Train on local

  ```bash
  # The parameter of training_shape define image shape for network, default is "".
  # It means use 10 kinds of shape as input shape, or it can be set some kind of shape.
  # run training example(1p) by python command.
  python train.py \
      --data_dir=./dataset/coco2017 \
      --pretrained_backbone=backbone_res2net50.ckpt \
      --is_distributed=0 \
      --lr=0.001 \
      --loss_scale=1024 \
      --weight_decay=0.016 \
      --T_max=320 \
      --max_epoch=320 \
      --warmup_epochs=4 \
      --training_shape=416 \
      --lr_scheduler=cosine_annealing > log.txt 2>&1 &

  # For Ascend device, standalone training example(1p) by shell script
  bash run_standalone_train.sh dataset/coco2017 backbone_res2net50.ckpt

  # For GPU device, standalone training example(1p) by shell script
  bash run_standalone_train_gpu.sh dataset/coco2017 backbone_res2net50.ckpt

  # For Ascend device, distributed training example(8p) by shell script
  bash run_distribute_train.sh dataset/coco2017 backbone_res2net50.ckpt hccl_8p.json

  # For GPU device, distributed training example(8p) by shell script
  bash run_distribute_train_gpu.sh dataset/coco2017 backbone_res2net50.ckpt

  # run evaluation by python command
    - For the standalone training mode, the ckpt file generated by training is stored in train/outputs/{year}-{month}-{day}_time_{hour}_{minute}_{second}/ckpt_0 directory.
    - For distributed training mode, the ckpt file generated by training is stored in train_parallel0/outputs/{year}-{month}-{day}_time_{hour}_{minute}_{second}/ckpt_0 directory.

  python eval.py \
      --data_dir=./dataset/coco2017 \
      --pretrained=train_parallel0/outputs/{year}-{month}-{day}_time_{hour}_{minute}_{second}/ckpt_0/0-99_31680.ckpt \
      --testing_shape=416 > log.txt 2>&1 &

  # run evaluation by shell script
  bash run_eval.sh dataset/coco2017/ train_parallel0/outputs/{year}-{month}-{day}_time_{hour}_{minute}_{second}/ckpt_0/0-99_31680.ckpt
  ```

## [Script Description](#contents)

### [Script and Sample Code](#contents)

```bash
.
└─yolov3_res2net50
  ├─README.md
  ├─mindspore_hub_conf.md             # config for mindspore hub
  ├─scripts
    ├─run_standalone_train.sh         # launch standalone training(1p) in ascend
    ├─run_distribute_train.sh         # launch distributed training(8p) in ascend
    └─run_eval.sh                     # launch evaluating in ascend
    ├─run_standalone_train_gpu.sh     # launch standalone training(1p) in gpu
    ├─run_distribute_train_gpu.sh     # launch distributed training(8p) in gpu
    └─run_eval_gpu.sh                 # launch evaluating in gpu
  ├─src
    ├─__init__.py                     # python init file
    ├─config.py                       # parameter configuration
    ├─res2net.py                      # backbone of network
    ├─distributed_sampler.py          # iterator of dataset
    ├─initializer.py                  # initializer of parameters
    ├─logger.py                       # log function
    ├─loss.py                         # loss function
    ├─lr_scheduler.py                 # generate learning rate
    ├─transforms.py                   # Preprocess data
    ├─util.py                         # util function
    ├─yolo.py                         # yolov3 network
    ├─yolo_dataset.py                 # create dataset for YOLOV3
  ├─eval.py                           # eval net
  ├─export.py                         # export mindir script
  ├─postprocess.py                         # inference result calculation script
  └─train.py                          # train net
```

### [Script Parameters](#contents)

```bash
Major parameters in train.py as follow.

optional arguments:
  -h, --help            show this help message and exit
  --device_target       device where the code will be implemented: "Ascend" | "GPU", default is "Ascend"
  --data_dir DATA_DIR   Train dataset directory.
  --per_batch_size PER_BATCH_SIZE
                        Batch size for Training. Default: 32.
  --pretrained_backbone PRETRAINED_BACKBONE
                        The ckpt file of res2net50. Default: "".
  --resume_yolov3 RESUME_YOLOV3
                        The ckpt file of YOLOv3, which used to fine tune.
                        Default: ""
  --lr_scheduler LR_SCHEDULER
                        Learning rate scheduler, options: exponential,
                        cosine_annealing. Default: exponential
  --lr LR               Learning rate. Default: 0.001
  --lr_epochs LR_EPOCHS
                        Epoch of changing of lr changing, split with ",".
                        Default: 220,250
  --lr_gamma LR_GAMMA   Decrease lr by a factor of exponential lr_scheduler.
                        Default: 0.1
  --eta_min ETA_MIN     Eta_min in cosine_annealing scheduler. Default: 0
  --T_max T_MAX         T-max in cosine_annealing scheduler. Default: 320
  --max_epoch MAX_EPOCH
                        Max epoch num to train the model. Default: 320
  --warmup_epochs WARMUP_EPOCHS
                        Warmup epochs. Default: 0
  --weight_decay WEIGHT_DECAY
                        Weight decay factor. Default: 0.0005
  --momentum MOMENTUM   Momentum. Default: 0.9
  --loss_scale LOSS_SCALE
                        Static loss scale. Default: 1024
  --label_smooth LABEL_SMOOTH
                        Whether to use label smooth in CE. Default:0
  --label_smooth_factor LABEL_SMOOTH_FACTOR
                        Smooth strength of original one-hot. Default: 0.1
  --log_interval LOG_INTERVAL
                        Logging interval steps. Default: 100
  --ckpt_path CKPT_PATH
                        Checkpoint save location. Default: outputs/
  --ckpt_interval CKPT_INTERVAL
                        Save checkpoint interval. Default: None
  --is_save_on_master IS_SAVE_ON_MASTER
                        Save ckpt on master or all rank, 1 for master, 0 for
                        all ranks. Default: 1
  --is_distributed IS_DISTRIBUTED
                        Distribute train or not, 1 for yes, 0 for no. Default:
                        1
  --rank RANK           Local rank of distributed. Default: 0
  --group_size GROUP_SIZE
                        World size of device. Default: 1
  --need_profiler NEED_PROFILER
                        Whether use profiler. 0 for no, 1 for yes. Default: 0
  --training_shape TRAINING_SHAPE
                        Fix training shape. Default: ""
  --resize_rate RESIZE_RATE
                        Resize rate for multi-scale training. Default: None
  --bind_cpu BIND_CPU
                        Whether bind cpu when distributed training. Default: True
  --device_num DEVICE_NUM
                        Device numbers per server. Default: 8
```

### [Training Process](#contents)

#### Distributed Training

For Ascend device, distributed training example(8p) by shell script

```bash
bash run_distribute_train.sh dataset/coco2017 backbone_res2net50.ckpt hccl_8p.json
```

For GPU device, distributed training example(8p) by shell script

```bash
bash run_distribute_train_gpu.sh dataset/coco2017 backbone_res2net50.ckpt
```

The above shell script will run distribute training in the background. You can view the results through the file `train_parallel0/log.txt`. The loss value will be achieved as follows:

```bash
# distribute training result(8p)
# grep "loss:" train/log.txt
2021-12-04 14:29:51,504:INFO:epoch[319], iter[146553], loss:19.998569, 482.50 imgs/sec, lr:2.4095520245737134e-08, per step time:530.5647850036621ms
2021-12-04 14:29:51,505:INFO:iter[146554], shape320
2021-12-04 14:29:51,701:INFO:epoch[319], iter[146554], loss:30.610958, 1300.71 imgs/sec, lr:2.4095520245737134e-08, per step time:196.81572914123535ms
2021-12-04 14:29:51,702:INFO:iter[146555], shape320
2021-12-04 14:29:51,888:INFO:epoch[319], iter[146555], loss:27.250893, 1376.75 imgs/sec, lr:2.4095520245737134e-08, per step time:185.9450340270996ms
2021-12-04 14:29:51,889:INFO:iter[146556], shape320
2021-12-04 14:29:52,089:INFO:epoch[319], iter[146556], loss:44.590981, 1272.75 imgs/sec, lr:2.4095520245737134e-08, per step time:201.1399269104004ms
2021-12-04 14:29:52,091:INFO:iter[146557], shape320
2021-12-04 14:29:52,281:INFO:epoch[319], iter[146557], loss:37.054596, 1338.08 imgs/sec, lr:2.4095520245737134e-08, per step time:191.31851196289062ms
2021-12-04 14:29:52,283:INFO:iter[146558], shape544
...
```

#### Standalone Training

For Ascend device, standalone training example(1p) by shell script

```bash
bash run_standalone_train.sh dataset/coco2017 backbone_res2net50.ckpt
```

For GPU device, standalone training example(1p) by shell script

```bash
bash run_standalone_train_gpu.sh dataset/coco2017 backbone_res2net50.ckpt
```

### [Evaluation Process](#contents)

#### Evaluation

Before running the command below. If running on GPU, please add `--device_target=GPU` in the python command or use the "_gpu" shell script ("xxx_gpu.sh").

```bash
python eval.py \
    --data_dir=./dataset/coco2017 \
    --pretrained=train_parallel0/outputs/{year}-{month}-{day}_time_{hour}_{minute}_{second}/ckpt_0/0-99_31680.ckpt \
    --testing_shape=416 > log.txt 2>&1 &
OR
bash run_eval.sh dataset/coco2017/ train_parallel0/outputs/{year}-{month}-{day}_time_{hour}_{minute}_{second}/ckpt_0/0-99_31680.ckpt
```

The above python command will run in the background. You can view the results through the file "log.txt". The mAP of the test dataset will be as follows:

This the standard format from `pycocotools`, you can refer to [cocodataset](https://cocodataset.org/#detection-eval) for more detail.

```bash
=============coco eval result=========
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323

Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.543

Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.335

Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.149

Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346

Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.476

Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.271

Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.414

Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.439

Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.245

Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.469

Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.592
```

## [Inference Process](#contents)

**Before inference, please refer to [MindSpore Inference with C++ Deployment Guide](https://gitee.com/mindspore/models/blob/master/utils/cpp_infer/README.md) to set environment variables.**

### [Export MindIR](#contents)

Currently, batchsize can only set to 1.

```bash
python export.py --ckpt_file [CKPT_PATH] --file_name [FILE_NAME] --file_format [FILE_FORMAT] --keep_detect [Bool]
```

The ckpt_file parameter is required,
Currently,`FILE_FORMAT` should be "MINDIR"
`keep_detect` keep the detect module or not, default: True

### [Infer on Ascend310](#contents)

Before performing inference, the mindir file must be exported by `export.py` script. We only provide an example of inference using MINDIR model.

```shell
# Ascend310 inference
bash run_infer_310.sh [MINDIR_PATH] [DATA_PATH] [ANNO_PATH] [DEVICE_ID]
```

- `MINDIR_PATH` specifies path of used "MINDIR" model.
- `DATA_PATH` specifies path of dataset.
- `ANNO_PATH` specifies path of annotation file.
- `DEVICE_ID` is optional, default value is 0.

### [Result](#contents)

Inference result is saved in current path, you can find result like this in acc.log file.

```bash
=============coco eval result=========
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303

Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.535

Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.310

Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.151

Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348

Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.423

Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.252

Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.402

Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.430

Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.267

Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.472

Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560
```

## [Model Description](#contents)

### [Performance](#contents)

#### Evaluation Performance

| Parameters                 | YOLO                                                        |
| -------------------------- | ----------------------------------------------------------- |
| Model Version              | YOLOv3                                                      |
| Resource                   |  Ascend 910; CPU 2.60GHz, 192cores; Memory 755G; OS Euler2.8|
| uploaded Date              | 12/5/2021 (month/day/year)                                  |
| MindSpore Version          | 1.5                                                         |
| Dataset                    | coco2017                                                    |
| Training Parameters        | epoch=320, batch_size=32, lr=0.001, momentum=0.9            |
| Optimizer                  | Momentum                                                    |
| Loss Function              | Sigmoid Cross Entropy with logits                           |
| outputs                    | boxes and label                                             |
| Loss                       | 25                                                          |
| Speed                      | 1pc: 382ms/step; 8pc: 506ms/step;                           |
| Total time                 | 8pc: 13 hours                                               |
| Parameters (M)             | 108.3M                                                      |
| Checkpoint for Fine tuning | 826M (.ckpt file)                                           |

## [Description of Random Situation](#contents)

There are random seeds in distributed_sampler.py, transforms.py, yolo_dataset.py files.

## [ModelZoo Homepage](#contents)

 Please check the official [homepage](https://gitee.com/mindspore/models).
