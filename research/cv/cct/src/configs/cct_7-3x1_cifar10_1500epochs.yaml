# Architecture Top1-77.6%
arch: cct_7_3x1_32

# ===== Dataset ===== #
data_url: ./data/cifar10
set: CIFAR10
num_classes: 10
mix_up: 0.8
cutmix: 1.0
auto_augment: rand-m9-mstd0.5-inc1
interpolation: bicubic
re_prob: 0.25  # for epoch 1500
re_mode: pixel
re_count: 1
mixup_prob: 1.
switch_prob: 0.5
mixup_mode: batch
mixup_off_epoch: 1500. # for epoch 1500


# ===== Learning Rate Policy ======== #
optimizer: adamw
base_lr: 0.0005 # for epoch 1500
warmup_lr: 0.00001
min_lr: 0.00001
lr_scheduler: cosine_lr
warmup_length: 10
nonlinearity: GELU


# ===== Network training config ===== #
amp_level: O2
keep_bn_fp32: True
beta: [ 0.9, 0.999 ]
clip_global_norm_value: 20.
is_dynamic_loss_scale: True
epochs: 1500
label_smoothing: 0.1
loss_scale: 1024
weight_decay: 0.05 # for epoch 1500
momentum: 0.9
batch_size: 128

# ===== Hardware setup ===== #
num_parallel_workers: 16
device_target: Ascend

# ===== Model config ===== #
drop_path_rate: 0.1
image_size: 32