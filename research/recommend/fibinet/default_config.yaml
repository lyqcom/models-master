# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly the meaning of each parameter)
enable_modelarts: False
data_url: ""
train_url: ""
checkpoint_url: ""
preprocess_data_path: "./data/"
data_path: "./data/mindrecord/"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: GPU
enable_profiling: False

# ==============================================================================
# argparse_init 'fibinet'
epochs: 10
full_batch: False
batch_size: 1000
eval_batch_size: 1000
field_size: 39
vocab_size: 200000
vocab_cache_size: 0
emb_dim: 10
deep_layer_dim: [400, 400, 400]
deep_layer_act: 'relu'
keep_prob: 0.5
dropout_flag: False
ckpt_path: "./ckpt/"
stra_ckpt: "./checkpoints"
eval_file_name: "eval.log"
loss_file_name: "loss.log"
dataset_type: "mindrecord"
field_slice: False
sparse: False
eval_while_train: False
deep_table_slice_mode: "column_slice"
seed: 1024
weight_bias_init: ['normal', 'normal']
emb_init: 'normal'
init_args: [-0.01, 0.01]
l2_coef: 0.00008 # 8e-5
manual_shape: None
reduction_ratio: 3
bilinear_type: "all"  #bilinear_type: ['all', 'each', 'interaction']

# ==============================================================================
# fibinet export
device_id: 0
ckpt_file: "./ckpt/fibinet_train-10_41322.ckpt"
file_name: "fibinet"
file_format: "MINDIR"

#'postprocess'
result_path: "./result_Files"
label_path: ''

# 'preprocess.'
dataset_path: ''
#result_path: './preprocess_Result'

# src/process_data.py  "Get and Process datasets"
raw_data_path: "./raw_data"

# src/preprocess_data.py  "Recommendation dataset used"
dense_dim: 13
slot_dim: 26
threshold: 100
train_line_count: 45840617
skip_id_convert: 0
line_per_sample: 1000
eval_size: 0.1

# src/generate_synthetic_data.py 'Generate Synthetic Data'
output_file: "./train.txt"
label_dim: 2
number_examples: 4000000
vocabulary_size: 400000000
random_slot_values: 0


---
# Config description for each option
enable_modelarts: 'Whether training on modelarts, default: False'
data_url: 'Dataset url for obs'
train_url: 'Training output url for obs'
preprocess_data_path: 'Dataset path for local used in preprocess stage'
data_path: 'Dataset path for local used in training and evaluation'
output_path: 'Training output path for local'

device_target: "device target, support Ascend, GPU and CPU."
dataset_path: 'Dataset path'
batch_size: "batch size"
ckpt_path: 'Checkpoint path'
eval_file_name: 'Auc log file path. Default: "./auc.log"'
loss_file_name: 'Loss log file path. Default: "./loss.log"'
do_eval: 'Do evaluation or not, only support "True" or "False". Default: "True"'
checkpoint_path: 'Checkpoint file path'
device_id: "Device id"
ckpt_file: "Checkpoint file path."
file_name: "output file name."
file_format: "file format"
result_path: 'Result path'
# result_path: "./result_Files" # 'result path'
label_path: 'label path'
dataset_type: "tfrecord/mindrecord/hd5"
field_slice: "Enable split field mode or not"
sparse: "Enable sparse or not"
deep_table_slice_mode: "column_slice/row_slice"

epochs: "Total train epochs"
full_batch: "Enable loading the full batch"
eval_batch_size: "Eval batch size."
field_size: "The number of features."
vocab_size: "The total features of dataset."
vocab_cache_size: "The total features of hash table."
emb_dim: "The dense embedding dimension of sparse feature."
deep_layer_dim: "The dimension of all deep layers."
deep_layer_act: "The activation function of all deep layers."
keep_prob: "The keep rate in dropout layer."
dropout_flag: "Enable dropout"
stra_ckpt: "The strategy checkpoint file."
eval_while_train: "Whether to evaluate after each epoch."
dense_dim: 'The number of your continues fields'
slot_dim: 'The number of your sparse fields, it can also be called catelogy features.'
threshold: 'Word frequency below this will be regarded as OOV. It aims to reduce the vocab size'
train_line_count: 'The number of examples in your dataset'
skip_id_convert: 'Skip the id convert, regarding the original id as the final id.'
line_per_sample: 'The number of sample per line, must be divisible by batch_size.'
eval_size: 'The percent of eval samples in the whole dataset'
raw_data_path: "The path to save dataset"
output_file: 'The output path of the generated file'
label_dim: 'The label category'
number_examples: 'The row numbers of the generated file'
vocabulary_size: "The vocabulary size of the total dataset"
random_slot_values: "If 1, the id is geneted by the random. If false, the id is set by the row_index mod part_size, where part_size the the vocab size for each slot"

---
device_target: ['Ascend', 'GPU', 'CPU']
file_format: ["AIR", "ONNX", "MINDIR"]
freeze_layer: ["", "none", "backbone"]
skip_id_convert: [0, 1]
dataset_type: ["tfrecord", "mindrecord", "hd5"]
