# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "Ascend"
enable_profiling: False
# ==============================================================================
# Parameters that can be modified at the terminal
# Train
train_data_dir: ''
ckpt_path: 'ckpts'
eval_file_name: "./auc.log"
loss_file_name: "./loss.log"
epochs: 10
do_eval: "True"
# Test
test_data_dir: ''
checkpoint_path: ''
# Export
batch_size: 16000
ckpt_file: ''
file_name: "edcn"
file_format: "MINDIR"
# 310infer related
dataset_path: ""
result_path: "./result_Files"
label_path: ""
# Dataset related
DataConfig:
    data_vocab_size: 184965
    train_num_of_parts: 21
    test_num_of_parts: 3
    batch_size: 1000
    data_field_size: 39
    # dataset format, 1: mindrecord, 2: tfrecord, 3: h5
    data_format: 1
# Model related
ModelConfig:
    batch_size: DataConfig.batch_size
    data_field_size: DataConfig.data_field_size
    data_vocab_size: DataConfig.data_vocab_size
    data_emb_dim: 80
    deep_layer_args: "relu"
    num_cross_layer: 3
    init_args: [-0.01, 0.01]
    weight_bias_init: ['normal', 'normal']
    keep_prob: 0.9
    temperature: 1.0
    batch_norm: True
# Training related
TrainConfig:
    batch_size: DataConfig.batch_size
    l1_coef: 0.000001
    l2_coef: 0.000005
    learning_rate: 0.0003
    epsilon: 0.00000001
    loss_scale: 1024.0
    save_checkpoint: True
    ckpt_file_name_prefix: "edcn"
    save_checkpoint_steps: 1
    keep_checkpoint_max: 15
    eval_callback: True
    loss_callback: True

# src/preprocess_data.py  "Recommendation dataset"
dense_dim: 13
slot_dim: 26
threshold: 100
train_line_count: 45840617
skip_id_convert: 0

---
# Help description for each configuration
# Parameters that been used on ModelArts
enable_modelarts: "Whether training on modelarts, default: False"
data_url: "Url for modelarts"
train_url: "Url for modelarts"
data_path: "The location of the input data."
output_path: "The location of the output file."
device_target: "Running platform, choose from Ascend, and default is Ascend."
enable_profiling: 'Whether enable profiling while training, default: False'
# Parameters that can be modified at the terminal
train_data_dir: "Train dataset dir, default is None"
ckpt_path: "ckpt dir to save, default is None"
eval_file_name: "Loss log file path. Default: './loss.log'"
loss_file_name: "Loss log file path. Default: './loss.log'"
epochs: "Training epochs. Default: 10"
do_eval: 'Do evaluation or not, only support "True" or "False". Default: "True"'
test_data_dir: "Test dataset dir, default is None"
checkpoint_path: " Relative path of '*.ckpt' to be evaluated relative to the eval.py"
ckpt_file: "Checkpoint file path."
file_name: "Output file name."
file_format: "Output file format, you can choose from AIR or MINDIR, default is MINDIR"
# Parameters that been used for preprocessing data
dense_dim: 'The number of your continues fields'
slot_dim: 'The number of your sparse fields, it can also be called catelogy features.'
threshold: 'Word frequency below this will be regarded as OOV. It aims to reduce the vocab size'
train_line_count: 'The number of examples in your dataset'
skip_id_convert: 'Skip the id convert, regarding the original id as the final id.'
---
#Choices
device_target: ["Ascend"]
do_eval: ["True", "False"]
file_format: ["AIR", "MINDIR"]
