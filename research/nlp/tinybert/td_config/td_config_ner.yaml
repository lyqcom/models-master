# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "Ascend"
enable_profiling: False

modelarts_dataset_unzip_name: ''
folder_name_under_zip_file: ''
# ==============================================================================
description: 'task_distill'
task_type: "classification"
task_name: ""
device_id: 0
# task_distill related
do_train: "true"
do_eval: "true"
td_phase1_epoch_size: 10
td_phase2_epoch_size: 3
do_shuffle: "true"
enable_data_sink: "true"
save_ckpt_step: 100
max_ckpt_num: 1
data_sink_steps: 1
load_teacher_ckpt_path: ""
load_gd_ckpt_path: ""
load_td1_ckpt_path: ""
train_data_dir: ""
eval_data_dir: ""
schema_dir: ""
assessment_method: "accuracy"
dataset_type: "tfrecord"
# export related
ckpt_file: ''
file_name: "tinybert"
file_format: "MINDIR"
num_labels: 43
seed1: 123
seed2: 1234
phase1_cfg:
    batch_size: 32
    loss_scale_value: 256
    scale_factor: 2
    scale_window: 50
    optimizer_cfg:
        AdamWeightDecay:
            learning_rate: 0.00005  # 5e-5
            end_learning_rate: 0.00000000000001  # 1e-14
            power: 1.0
            weight_decay: 0.0001  # 1e-4
            eps: 0.000001  # 1e-6
            decay_filter: ['layernorm', 'bias']

phase2_cfg:
    batch_size: 32
    loss_scale_value: 65536
    scale_factor: 2
    scale_window: 50
    optimizer_cfg:
        AdamWeightDecay:
            learning_rate: 0.00002  # 5e-5
            end_learning_rate: 0.00000000000001  # 1e-14
            power: 1.0
            weight_decay: 0.0001  # 1e-4
            eps: 0.000001  # 1e-6
            decay_filter: ['layernorm', 'bias']

eval_cfg:
    batch_size: 32

td_teacher_net_cfg:
    seq_length: 128
    vocab_size: 21128
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: False
    dtype: mstype.float32
    compute_type: mstype.float16

td_student_net_cfg:
    seq_length: 128
    vocab_size: 21128
    hidden_size: 384
    num_hidden_layers: 4
    num_attention_heads: 12
    intermediate_size: 1536
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: False
    dtype: mstype.float32
    compute_type: mstype.float16

---

# Help description for each configuration
enable_modelarts: "Whether training on modelarts, default: False"
data_url: "Url for modelarts"
train_url: "Url for modelarts"
data_path: "The location of the input data."
output_path: "The location of the output file."
device_target: "Running platform, choose from Ascend, GPU or CPU, and default is Ascend."
enable_profiling: 'Whether enable profiling while training, default: False'
modelarts_dataset_unzip_name: ''
folder_name_under_zip_file: ''
# task_distill related
do_train: "Do train task, default is true."
do_eval: "Do eval task, default is true."
td_phase1_epoch_size: "Epoch size for td phase 1, default is 10."
td_phase2_epoch_size: "Epoch size for td phase 2, default is 3."
device_id: "Device id, default is 0."
do_shuffle: "Enable shuffle for dataset, default is true."
enable_data_sink: "Enable data sink, default is true."
save_ckpt_step: ""
max_ckpt_num: "Enable data sink, default is true."
data_sink_steps: "Sink steps for each epoch, default is 1."
load_teacher_ckpt_path: "Load checkpoint file path"
load_gd_ckpt_path: "Load checkpoint file path"
load_td1_ckpt_path: "Load checkpoint file path"
train_data_dir: "Data path, it is better to use absolute path"
eval_data_dir: "Data path, it is better to use absolute path"
schema_dir: "Schema path, it is better to use absolute path"
task_type: "The type of the task to train."
task_name: "The name of the task to train."
assessment_method: "assessment_method include: [accuracy, bf1, mf1], default is accuracy"
dataset_type: "dataset type tfrecord/mindrecord, default is tfrecord"
# export related
ckpt_file: "tinybert ckpt file."
file_name: "output file name."
file_format: "file format"
# postprocess related
result_path: "result path"
label_path: "label path"
---
device_target: ['Ascend', 'GPU', 'CPU']
do_train: ["true", "false"]
do_eval: ["true", "false"]
do_shuffle: ["true", "false"]
enable_data_sink: ["true", "false"]
task_type: ["classification", "ner"]
task_name: ["SST-2", "QNLI", "MNLI", "TNEWS", "CLUENER"]
assessment_method: ["accuracy", "bf1", "mf1"]
file_format: ["AIR", "ONNX", "MINDIR"]
