# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
data_url: "/cache/egs/"
train_url: "/cache/CheckPoints/"
checkpoint_url: ""
data_path: "/cache/data"
output_path: "/cache/training"
load_path: "/cache/checkpoint_path"
checkpoint_path: ''
device_target: Ascend
enable_profiling: False

# ==============================================================================
# config/cfg edict
transformer_network: 'base'
init_loss_scale_value: 1024
scale_factor: 2
scale_window: 2000
optimizer: 'Adam'
lr_param_k: 0.2
warmup_steps: 4000
optimizer_adam_beta2: 0.997

# transformer_net_cfg
batch_size: 32
seq_length: 512
input_feature_size: 320
vocab_size: 4233
hidden_size: 512
num_hidden_layers: 6
num_attention_heads: 8
intermediate_size: 2048
hidden_act: "relu"
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
max_position_embeddings: 512
initializer_range: 0.02
label_smoothing: 0.1
dtype: mstype.float32
compute_type: mstype.float16

#eval_config/cfg edict
data_file: '/cache/data'
model_file: './transformer/transformer_trained.ckpt'
output_file: './output_eval.json'
data_json_path: '/path/to/egs/aishell/dump/train/deltafalse/data.json'
chars_dict_path: '/path/to/egs/aishell/data/lang_1char/train_chars.txt'

# transformer_net_cfg
batch_size_ev: 1
hidden_dropout_prob_ev: 0.0
attention_probs_dropout_prob_ev: 0.0
beam_width: 5
max_decode_length: 100
length_penalty_weight: 1.0

# ==============================================================================
# train.py / Argparse init.
distribute: "false"
epoch_size: 91
device_id: 0
device_num: 1
enable_lossscale: "true"
do_shuffle: "true"
enable_save_ckpt: "true"
save_checkpoint_steps: 3753
save_checkpoint_num: 30
save_checkpoint_path: "./"
accumulation_steps: 1

# export.py /eval_config - transformer export
file_name: "transformer"
file_format: 'MINDIR'

#'postprocess / from eval_config'
result_dir: "./result_Files"

#'preprocess / from eval_config'
result_path: "./preprocess_Result/"

# src/process_output.py "recore nbest with smoothed sentence-level bleu."
vocab_file: ""

---
# Config description for each option
enable_modelarts: 'Whether training on modelarts, default: False'
data_url: 'Dataset url for obs'
train_url: 'Training output url for obs'
data_path: 'Dataset path for local, it is better to use absolute path'
output_path: 'Training output path for local'
ann_file: 'Ann file, default is val.json.'

device_target: "device where the code will be implemented, default is Ascend"
checkpoint_path: "Checkpoint file path"
data_file: '/your/path/evaluation.mindrecord'
model_file: '/your/path/checkpoint_file'
output_file: './output_eval.txt'
data_json_path: '/path/to/egs/aishell/pickled_dataset/train/data.json  or  /path/to/egs/aishell/pickled_dataset/test/data.json'
chars_dict_path: '/path/to/egs/aishell/data/lang_1char/train_chars.txt'

distribute: "Run distribute, default is false."
epoch_size: "Epoch size, default is 91（1p），279(8p)"
device_id: "Device id, default is 0."
device_num: "Use device nums, default is 1."
enable_lossscale: "Use lossscale or not, default is true."
do_shuffle: "Enable shuffle for dataset, default is true."
enable_save_ckpt: "Enable save checkpoint, default is true."
save_checkpoint_steps: "Save checkpoint steps, default is 3753."
save_checkpoint_num: "Save checkpoint numbers, default is 30."
save_checkpoint_path: "Save checkpoint file path"
accumulation_steps: "Gradient accumulation steps, default is 1."

file_name: "output file name."
file_format: 'file format'
result_dir: "./result_Files"
result_path: "./preprocess_Result/"
vocab_file: "vocab file path."
input_file: 'Input raw text file (or comma-separated list of files).'
num_splits: 'The MindRecord file will be split into the number of partition.'
clip_to_max_len: 'clip sequences to maximum sequence length.'
max_seq_length: 'Maximum sequence length.'

---
device_target: ["Ascend", "GPU", "CPU"]
file_format: ["AIR", "ONNX", "MINDIR"]
distribute: ['true', 'false']
enable_lossscale: ['true', 'false']
do_shuffle: ['true', 'false']
enable_save_ckpt: ['true', 'false']