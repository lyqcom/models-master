# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
data_url: ""
train_url: ""
checkpoint_url: ""
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: Ascend
enable_profiling: False

# ==============================================================================
# config

dataset: ""
dataset_path: ""
truth_labels_path: ""
dev_dataset: ""
dev_data_path: ""
test_dataset: ""
test_data_path: ""
pretrain_ckpt_path: ""
save_checkpoint_path: "./"
ckpt_path_doc: ""
ckpt_partition: 8
distribute: False
has_train_strategy: True
result_path: ""
ckpt_epoch: 4
multi_machine: False

file_name: "cpm"
file_format: "MINDIR" # ["AIR", "MINDIR"]

config_zero_shot_standalone:
    dp: 1
    mp: 1
    batch_size: 1
    rank_size: 1
    vocab_size: 30000
    seq_length: 571
    hidden_size: 2560
    num_hidden_layers: 32
    num_attention_heads: 32

config_zero_shot_distrubute:
    dp: 1
    mp: 2
    batch_size: 2
    rank_size: 2
    vocab_size: 30000
    seq_length: 571
    hidden_size: 2560
    num_hidden_layers: 32
    num_attention_heads: 32

finetune_dev_standalone:
    dp: 1
    mp: 2
    batch_size: 2
    rank_size: 2
    vocab_size: 30000
    seq_length: 696
    hidden_size: 2560
    num_hidden_layers: 32
    num_attention_heads: 32

finetune_dev_distrubute:
    dp: 1
    mp: 2
    batch_size: 1
    rank_size: 2
    vocab_size: 30000
    seq_length: 696
    hidden_size: 2560
    num_hidden_layers: 32
    num_attention_heads: 32

finetune_test_standalone:
    dp: 1
    mp: 1
    batch_size: 1
    rank_size: 1
    vocab_size: 30000
    seq_length: 666
    hidden_size: 2560
    num_hidden_layers: 32
    num_attention_heads: 32

finetune_test_distrubute:
    dp: 1
    mp: 2
    batch_size: 1
    rank_size: 2
    vocab_size: 30000
    seq_length: 666
    hidden_size: 2560
    num_hidden_layers: 32
    num_attention_heads: 32

config_train_single_machine:
    dp: 4
    mp: 2
    epoch: 10
    batch_size: 16
    rank_size: 8
    vocab_size: 30000
    seq_length: 725
    hidden_size: 2560
    num_hidden_layers: 32
    num_attention_heads: 32
    lr: 0.00001
    eps: 0.00000001
    dropout: 0.2
    end_learning_rate: 0.0000001
    weight_decay: 0.01
    warmup_steps: 0.05
    power: 1.0
    grad_accumulation_step: 4
    sink_size: 1

config_train_multi_machine:
    dp: 16
    mp: 2
    epoch: 10
    batch_size: 128
    rank_size: 32
    vocab_size: 30000
    seq_length: 725
    hidden_size: 2560
    num_hidden_layers: 32
    num_attention_heads: 32
    lr: 0.00002
    eps: 0.00000001
    dropout: 0.1
    end_learning_rate: 0.0000001
    weight_decay: 0.01
    warmup_steps: 0.1
    power: 1.0
    grad_accumulation_step: 1
    sink_size: 1
